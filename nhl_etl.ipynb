{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "j6vgaiLGPpxc"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import time\n",
    "from functools import reduce\n",
    "from typing import List\n",
    "\n",
    "import dask\n",
    "import distributed\n",
    "import kagglehub\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dask import dataframe as dd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Blocksize and partition size for dask dataframe\n",
    "# https://docs.dask.org/en/stable/generated/dask.dataframe.read_csv.html\n",
    "# https://docs.dask.org/en/stable/dataframe-create.html#read-from-csv\n",
    "DASK_BLOCKSIZE = \"64MB\"  # 64MB seems to be the best\n",
    "# Name of the environment variable stored in ./.env for your sql uri\n",
    "# Easily switch to cloud server here e.g. \"URI_AZURE\"\n",
    "ENV_SQL_URI = \"URI_PG\"\n",
    "# Chunksize for to_sql. reduce value if you run out of memory\n",
    "CHUNKSIZE = 10000\n",
    "# Path to export data\n",
    "EXPORT_PATH = \"./.export\"\n",
    "\n",
    "# Configure logger and logging\n",
    "logger = logging.getLogger(__name__)\n",
    "LOG_FORMAT = \"{levelname}:{name}:{funcName}:{message}\"\n",
    "logging.basicConfig(level=logging.INFO, format=LOG_FORMAT, style=\"{\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from ./.env\n",
    "is_dotenv = load_dotenv()\n",
    "if not is_dotenv:  # remind user to create a .env\n",
    "    raise Exception(f\"Please create a .env and define the {ENV_SQL_URI} variable\")\n",
    "# Check if sql uri exists\n",
    "sql_uri = str(os.getenv(ENV_SQL_URI))\n",
    "if sql_uri == \"None\":\n",
    "    raise Exception(f\"Please define an sql uri on key {ENV_SQL_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set temporary directory so it doesn't burn out my SSD\n",
    "dask.config.set({\"temporary_directory\": \"./.tmp\"})  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the dask distributed client\n",
    "# Click on the link in output to see the dashboard\n",
    "distributed.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset to project folder\n",
    "# https://github.com/Kaggle/kagglehub/issues/175\n",
    "os.environ[\"KAGGLEHUB_CACHE\"] = \"./.kaggle\"\n",
    "dataset_path = kagglehub.dataset_download(\"martinellis/nhl-game-data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column dtypes for each file\n",
    "\n",
    "# dtype shortcuts\n",
    "DTYPE_BOOL = \"bool[pyarrow]\"\n",
    "DTYPE_INT = \"int64[pyarrow]\"  # note that pyarrow ints are nullable\n",
    "DTYPE_INT8 = \"int8[pyarrow]\"\n",
    "DTYPE_INT16 = \"int16[pyarrow]\"\n",
    "DTYPE_INT32 = \"int32[pyarrow]\"\n",
    "DTYPE_FLOAT = \"float64[pyarrow]\"\n",
    "DTYPE_STRING = \"string[pyarrow]\"\n",
    "DTYPE_DATETIME = \"timestamp[s][pyarrow]\"\n",
    "\n",
    "# Table definitions\n",
    "dtype_defs = {}\n",
    "dtype_defs[\"game.csv\"] = {\n",
    "    \"game_id\": DTYPE_INT,\n",
    "    \"season\": DTYPE_INT,\n",
    "    \"type\": DTYPE_STRING,\n",
    "    \"date_time_GMT\": DTYPE_DATETIME,\n",
    "    \"away_team_id\": DTYPE_INT8,\n",
    "    \"home_team_id\": DTYPE_INT8,\n",
    "    \"away_goals\": DTYPE_INT8,\n",
    "    \"home_goals\": DTYPE_INT8,\n",
    "    \"outcome\": DTYPE_STRING,\n",
    "    \"home_rink_side_start\": DTYPE_STRING,\n",
    "    \"venue\": DTYPE_STRING,\n",
    "    \"venue_link\": DTYPE_STRING,\n",
    "    \"venue_time_zone_id\": DTYPE_STRING,\n",
    "    \"venue_time_zone_offset\": DTYPE_INT8,\n",
    "    \"venue_time_zone_tz\": DTYPE_STRING,\n",
    "}\n",
    "dtype_defs[\"game_goalie_stats.csv\"] = {\n",
    "    \"game_id\": DTYPE_INT,\n",
    "    \"player_id\": DTYPE_INT32,\n",
    "    \"team_id\": DTYPE_INT8,\n",
    "    \"timeOnIce\": DTYPE_INT16,\n",
    "    \"assists\": DTYPE_INT8,\n",
    "    \"goals\": DTYPE_INT8,\n",
    "    \"pim\": DTYPE_INT8,\n",
    "    \"shots\": DTYPE_INT8,\n",
    "    \"saves\": DTYPE_INT8,\n",
    "    \"powerPlaySaves\": DTYPE_INT8,\n",
    "    \"shortHandedSaves\": DTYPE_INT8,\n",
    "    \"evenSaves\": DTYPE_INT8,\n",
    "    \"shortHandedShotsAgainst\": DTYPE_INT8,\n",
    "    \"evenShotsAgainst\": DTYPE_INT8,\n",
    "    \"powerPlayShotsAgainst\": DTYPE_INT8,\n",
    "    \"decision\": DTYPE_STRING,\n",
    "    \"savePercentage\": DTYPE_FLOAT,\n",
    "    \"powerPlaySavePercentage\": DTYPE_FLOAT,\n",
    "    \"evenStrengthSavePercentage\": DTYPE_FLOAT,\n",
    "}\n",
    "dtype_defs[\"game_goals.csv\"] = {\n",
    "    \"play_id\": DTYPE_STRING,\n",
    "    \"strength\": DTYPE_STRING,\n",
    "    \"gameWinningGoal\": DTYPE_BOOL,\n",
    "    \"emptyNet\": DTYPE_BOOL,\n",
    "}\n",
    "dtype_defs[\"game_officials.csv\"] = {\n",
    "    \"game_id\": DTYPE_INT,\n",
    "    \"official_name\": DTYPE_STRING,\n",
    "    \"official_type\": DTYPE_STRING,\n",
    "}\n",
    "dtype_defs[\"game_penalties.csv\"] = {\n",
    "    \"play_id\": DTYPE_STRING,\n",
    "    \"penaltySeverity\": DTYPE_STRING,\n",
    "    \"penaltyMinutes\": DTYPE_INT8,\n",
    "}\n",
    "dtype_defs[\"game_plays.csv\"] = {\n",
    "    \"play_id\": DTYPE_STRING,\n",
    "    \"game_id\": DTYPE_INT,\n",
    "    \"team_id_for\": DTYPE_INT8,\n",
    "    \"team_id_against\": DTYPE_INT8,\n",
    "    \"event\": DTYPE_STRING,\n",
    "    \"secondaryType\": DTYPE_STRING,\n",
    "    \"x\": DTYPE_INT8,\n",
    "    \"y\": DTYPE_INT8,\n",
    "    \"period\": DTYPE_INT8,\n",
    "    \"periodType\": DTYPE_STRING,\n",
    "    \"periodTime\": DTYPE_INT16,\n",
    "    \"periodTimeRemaining\": DTYPE_INT16,\n",
    "    \"dateTime\": DTYPE_DATETIME,\n",
    "    \"goals_away\": DTYPE_INT8,\n",
    "    \"goals_home\": DTYPE_INT8,\n",
    "    \"description\": DTYPE_STRING,\n",
    "    \"st_x\": DTYPE_INT8,\n",
    "    \"st_y\": DTYPE_INT8,\n",
    "}\n",
    "dtype_defs[\"game_plays_players.csv\"] = {\n",
    "    \"play_id\": DTYPE_STRING,\n",
    "    \"game_id\": DTYPE_INT,\n",
    "    \"player_id\": DTYPE_INT32,\n",
    "    \"playerType\": DTYPE_STRING,\n",
    "}\n",
    "dtype_defs[\"game_scratches.csv\"] = {\n",
    "    \"game_id\": DTYPE_INT,\n",
    "    \"team_id\": DTYPE_INT8,\n",
    "    \"player_id\": DTYPE_INT32,\n",
    "}\n",
    "dtype_defs[\"game_shifts.csv\"] = {\n",
    "    \"game_id\": DTYPE_INT,\n",
    "    \"player_id\": DTYPE_INT32,\n",
    "    \"period\": DTYPE_INT8,\n",
    "    \"shift_start\": DTYPE_INT16,\n",
    "    \"shift_end\": DTYPE_INT16,\n",
    "}\n",
    "dtype_defs[\"game_skater_stats.csv\"] = {\n",
    "    \"game_id\": DTYPE_INT,\n",
    "    \"player_id\": DTYPE_INT32,\n",
    "    \"team_id\": DTYPE_INT8,\n",
    "    \"timeOnIce\": DTYPE_INT16,\n",
    "    \"assists\": DTYPE_INT8,\n",
    "    \"goals\": DTYPE_INT8,\n",
    "    \"shots\": DTYPE_INT8,\n",
    "    \"hits\": DTYPE_INT8,\n",
    "    \"powerPlayGoals\": DTYPE_INT8,\n",
    "    \"powerPlayAssists\": DTYPE_INT8,\n",
    "    \"penaltyMinutes\": DTYPE_INT8,\n",
    "    \"faceOffWins\": DTYPE_INT8,\n",
    "    \"faceoffTaken\": DTYPE_INT8,\n",
    "    \"takeaways\": DTYPE_INT8,\n",
    "    \"giveaways\": DTYPE_INT8,\n",
    "    \"shortHandedGoals\": DTYPE_INT8,\n",
    "    \"shortHandedAssists\": DTYPE_INT8,\n",
    "    \"blocked\": DTYPE_INT8,\n",
    "    \"plusMinus\": DTYPE_INT8,\n",
    "    \"evenTimeOnIce\": DTYPE_INT16,\n",
    "    \"shortHandedTimeOnIce\": DTYPE_INT16,\n",
    "    \"powerPlayTimeOnIce\": DTYPE_INT16,\n",
    "}\n",
    "dtype_defs[\"game_teams_stats.csv\"] = {\n",
    "    \"game_id\": DTYPE_INT,\n",
    "    \"team_id\": DTYPE_INT8,\n",
    "    \"HoA\": DTYPE_STRING,\n",
    "    \"won\": DTYPE_BOOL,\n",
    "    \"settled_in\": DTYPE_STRING,\n",
    "    \"head_coach\": DTYPE_STRING,\n",
    "    \"goals\": DTYPE_INT8,\n",
    "    \"shots\": DTYPE_INT8,\n",
    "    \"hits\": DTYPE_INT8,\n",
    "    \"pim\": DTYPE_INT16,\n",
    "    \"powerPlayOpportunities\": DTYPE_INT8,\n",
    "    \"powerPlayGoals\": DTYPE_INT8,\n",
    "    \"faceOffWinPercentage\": DTYPE_FLOAT,\n",
    "    \"giveaways\": DTYPE_INT8,\n",
    "    \"takeaways\": DTYPE_INT8,\n",
    "    \"blocked\": DTYPE_INT8,\n",
    "    \"startRinkSide\": DTYPE_STRING,\n",
    "}\n",
    "dtype_defs[\"player_info.csv\"] = {\n",
    "    \"player_id\": DTYPE_INT32,\n",
    "    \"firstName\": DTYPE_STRING,\n",
    "    \"lastName\": DTYPE_STRING,\n",
    "    \"nationality\": DTYPE_STRING,\n",
    "    \"birthCity\": DTYPE_STRING,\n",
    "    \"primaryPosition\": DTYPE_STRING,\n",
    "    \"birthDate\": DTYPE_DATETIME,\n",
    "    \"birthStateProvince\": DTYPE_STRING,\n",
    "    \"height\": DTYPE_STRING,\n",
    "    \"height_cm\": DTYPE_FLOAT,\n",
    "    \"weight\": DTYPE_INT16,\n",
    "    \"shootsCatches\": DTYPE_STRING,\n",
    "}\n",
    "dtype_defs[\"team_info.csv\"] = {\n",
    "    \"team_id\": DTYPE_INT8,\n",
    "    \"franchiseId\": DTYPE_INT8,\n",
    "    \"shortName\": DTYPE_STRING,\n",
    "    \"teamName\": DTYPE_STRING,\n",
    "    \"abbreviation\": DTYPE_STRING,\n",
    "    \"link\": DTYPE_STRING,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HZint-P3TbkH",
    "outputId": "b1571a3b-dc0b-4d33-b672-bce2f553e30e"
   },
   "outputs": [],
   "source": [
    "# Read all csv files from data directory\n",
    "dfs: dict[str, dd.DataFrame] = {}\n",
    "for n in os.listdir(dataset_path):\n",
    "    if n.endswith(\".csv\"):\n",
    "        if n in dtype_defs:\n",
    "            logger.info(f\"Reading {n} with dtypes\")\n",
    "            fpath = os.path.join(dataset_path, n)\n",
    "            dfs[n.split(\".\")[0]] = dd.read_csv(\n",
    "                fpath,\n",
    "                blocksize=DASK_BLOCKSIZE,\n",
    "                dtype=dtype_defs[n],\n",
    "                engine=\"pyarrow\",\n",
    "                dtype_backend=\"pyarrow\",\n",
    "            )\n",
    "        else:\n",
    "            logger.info(f\"Reading {n} without dtypes\")\n",
    "            fpath = os.path.join(dataset_path, n)\n",
    "            dfs[n.split(\".\")[0]] = dd.read_csv(\n",
    "                fpath,\n",
    "                blocksize=DASK_BLOCKSIZE,\n",
    "                engine=\"pyarrow\",\n",
    "                dtype_backend=\"pyarrow\",\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print partitions\n",
    "for k in dfs:\n",
    "    logger.info(f\"npartitions {k}: {dfs[k].npartitions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop duplicate rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicates(df: dd.DataFrame, subset: str) -> dd.DataFrame:\n",
    "    \"\"\"Drop duplicates and return the dataframe\"\"\"\n",
    "    rows_before = df.shape[0].compute()\n",
    "    df_deduped = df.drop_duplicates(subset=subset)\n",
    "    rows_after = df_deduped.shape[0].compute()\n",
    "    logger.info(f\"Dropped {rows_before - rows_after} rows using {subset}\")\n",
    "    return df_deduped\n",
    "\n",
    "\n",
    "# I have verified that data with the same key are duplicate rows\n",
    "dfs[\"game\"] = drop_duplicates(dfs[\"game\"], \"game_id\")\n",
    "dfs[\"game_plays\"] = drop_duplicates(dfs[\"game_plays\"], \"play_id\")\n",
    "dfs[\"player_info\"] = drop_duplicates(dfs[\"player_info\"], \"player_id\")\n",
    "dfs[\"team_info\"] = drop_duplicates(dfs[\"team_info\"], \"team_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop unreferenced data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unreferenced data that is unrecoverable\n",
    "def drop_unreferenced(\n",
    "    df_foreign: dd.DataFrame,\n",
    "    df_foreign_col: str,\n",
    "    df_primary: dd.DataFrame,\n",
    "    df_primary_col: str,\n",
    ") -> dd.DataFrame:\n",
    "    \"\"\"Drop unreferenced keys and returns the dataframe\"\"\"\n",
    "    rows_before = df_foreign.shape[0].compute()\n",
    "    # reduce memory usage with partitions\n",
    "    selections = []\n",
    "    for i in range(df_primary.npartitions):\n",
    "        selections.append(\n",
    "            df_foreign[df_foreign_col].isin(df_primary[df_primary_col].get_partition(i))\n",
    "        )\n",
    "    selector = reduce(lambda x, y: x | y, selections)\n",
    "    df_foreign = df_foreign[selector]\n",
    "    rows_after = df_foreign.shape[0].compute()\n",
    "    logger.info(f\"Dropped {rows_before - rows_after} rows on {df_foreign_col}\")\n",
    "    return df_foreign\n",
    "\n",
    "\n",
    "dfs[\"game_goalie_stats\"] = drop_unreferenced(\n",
    "    dfs[\"game_goalie_stats\"], \"team_id\", dfs[\"team_info\"], \"team_id\"\n",
    ")\n",
    "dfs[\"game_plays_players\"] = drop_unreferenced(\n",
    "    dfs[\"game_plays_players\"], \"play_id\", dfs[\"game_plays\"], \"play_id\"\n",
    ")\n",
    "dfs[\"game_skater_stats\"] = drop_unreferenced(\n",
    "    dfs[\"game_skater_stats\"], \"team_id\", dfs[\"team_info\"], \"team_id\"\n",
    ")\n",
    "dfs[\"game_teams_stats\"] = drop_unreferenced(\n",
    "    dfs[\"game_teams_stats\"], \"team_id\", dfs[\"team_info\"], \"team_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add summary statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmean(df: dd.DataFrame, cols: List[str]):\n",
    "    \"\"\"Calculate the geometric mean across a list of columns `cols` in `df`\"\"\"\n",
    "    return df[cols].prod(axis=1) ** (1 / len(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'won' from bool to int allows us to perform mean() and calculate the win rate\n",
    "dfs[\"game_teams_stats\"][\"winRate\"] = dfs[\"game_teams_stats\"][\"won\"].astype(\n",
    "    \"int8[pyarrow]\"\n",
    ")\n",
    "# Generate average team stats from per-game team stats\n",
    "dfs[\"avg_teams_stats\"] = (\n",
    "    dfs[\"game_teams_stats\"].groupby(\"team_id\").mean(numeric_only=True)\n",
    ")\n",
    "dfs[\"avg_teams_stats\"] = dfs[\"avg_teams_stats\"].drop(\"game_id\", axis=1)\n",
    "dfs[\"avg_teams_stats\"] = dfs[\"avg_teams_stats\"].reset_index()\n",
    "# Cleanup columns\n",
    "dfs[\"game_teams_stats\"] = dfs[\"game_teams_stats\"].drop(\"winRate\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Corsi and Fenwick per game\n",
    "\n",
    "# Self-join to allow cross-comparison of shots\n",
    "cols = [\"game_id\", \"team_id\", \"shots\", \"blocked\"]\n",
    "df_a = dfs[\"game_teams_stats\"][cols].rename(columns={\"team_id\": \"team_id_a\"})\n",
    "df_b = dfs[\"game_teams_stats\"][cols].rename(columns={\"team_id\": \"team_id_b\"})\n",
    "df = dd.merge(df_a, df_b, on=\"game_id\", suffixes=(\"_a\", \"_b\"))\n",
    "# Cleanup duplicates after self-join\n",
    "df = df[df[\"team_id_a\"] != df[\"team_id_b\"]]\n",
    "# Calculate Corsi and Fenwick\n",
    "df[\"corsi_a\"] = df[\"shots_a\"] - df[\"shots_b\"]\n",
    "df[\"fenwick_a\"] = df[\"shots_a\"] - df[\"blocked_a\"] - (df[\"shots_b\"] - df[\"blocked_b\"])\n",
    "# Use smallest int that fits the min and max to save memory\n",
    "df[\"corsi_a\"] = df[\"corsi_a\"].astype(\"int8[pyarrow]\")\n",
    "df[\"fenwick_a\"] = df[\"fenwick_a\"].astype(\"int8[pyarrow]\")\n",
    "# Cleanup redundant columns\n",
    "cols = [\"shots_a\", \"blocked_a\", \"team_id_b\", \"shots_b\", \"blocked_b\"]\n",
    "df = df.drop(cols, axis=1)\n",
    "# Remove suffixes\n",
    "cols = {\"team_id_a\": \"team_id\", \"corsi_a\": \"corsi\", \"fenwick_a\": \"fenwick\"}\n",
    "df = df.rename(columns=cols)\n",
    "# Save to dataframe collection\n",
    "dfs[\"game_teams_corfen\"] = df\n",
    "\n",
    "# Compute the average Corsi and Fenwick per team and save to avg_teams_stats\n",
    "df = df.groupby(\"team_id\").mean(numeric_only=True)\n",
    "df = df.reset_index()\n",
    "df = df.drop(\"game_id\", axis=1)\n",
    "dfs[\"avg_teams_stats\"] = dd.merge(\n",
    "    dfs[\"avg_teams_stats\"], df, how=\"left\", on=\"team_id\", suffixes=(\"\", \"_right\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate average skater stats from per-game skater stats\n",
    "dfs[\"avg_skater_stats\"] = (\n",
    "    dfs[\"game_skater_stats\"].groupby(\"player_id\").mean(numeric_only=True)\n",
    ")\n",
    "dfs[\"avg_skater_stats\"] = dfs[\"avg_skater_stats\"].drop([\"game_id\", \"team_id\"], axis=1)\n",
    "dfs[\"avg_skater_stats\"] = dfs[\"avg_skater_stats\"].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the decision string to int to allow the mean to calculate the win rate\n",
    "def decision_to_num(decision):\n",
    "    if isinstance(decision, str):\n",
    "        if decision == \"W\":\n",
    "            return 1\n",
    "        elif decision == \"L\":\n",
    "            return 0\n",
    "        else:\n",
    "            raise Exception(f\"Unexpected string '{decision}'\")\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "dfs[\"game_goalie_stats\"][\"decisionWinRate\"] = dfs[\"game_goalie_stats\"][\n",
    "    \"decision\"\n",
    "].apply(decision_to_num, meta=(0, \"int8[pyarrow]\"))\n",
    "\n",
    "# Generate average goalie stats from per-game goalie stats\n",
    "dfs[\"avg_goalie_stats\"] = (\n",
    "    dfs[\"game_goalie_stats\"].groupby(\"player_id\").mean(numeric_only=True)\n",
    ")\n",
    "dfs[\"avg_goalie_stats\"] = dfs[\"avg_goalie_stats\"].drop([\"game_id\", \"team_id\"], axis=1)\n",
    "dfs[\"avg_goalie_stats\"] = dfs[\"avg_goalie_stats\"].reset_index()\n",
    "\n",
    "# Drop the redundant column from per-game stats\n",
    "dfs[\"game_goalie_stats\"] = dfs[\"game_goalie_stats\"].drop(\"decisionWinRate\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the geometric mean of positive-meaning stats in avg_teams_stats\n",
    "# Note that we cannot include the Corsi and Fenwick as they have negative values\n",
    "cols = [\n",
    "    \"goals\",\n",
    "    \"shots\",\n",
    "    \"hits\",\n",
    "    \"powerPlayOpportunities\",\n",
    "    \"powerPlayGoals\",\n",
    "    \"faceOffWinPercentage\",\n",
    "    \"takeaways\",\n",
    "    \"blocked\",\n",
    "]\n",
    "dfs[\"avg_teams_stats\"][\"gmean_stats\"] = gmean(dfs[\"avg_teams_stats\"], cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the geometric mean of positive-meaning stats in avg_skater_stats\n",
    "cols = [\n",
    "    \"timeOnIce\",\n",
    "    \"assists\",\n",
    "    \"goals\",\n",
    "    \"shots\",\n",
    "    \"hits\",\n",
    "    \"powerPlayGoals\",\n",
    "    \"powerPlayAssists\",\n",
    "    \"penaltyMinutes\",\n",
    "    \"faceOffWins\",\n",
    "    \"faceoffTaken\",\n",
    "    \"takeaways\",\n",
    "    \"shortHandedGoals\",\n",
    "    \"shortHandedAssists\",\n",
    "    \"blocked\",\n",
    "    \"plusMinus\",\n",
    "    \"evenTimeOnIce\",\n",
    "    \"shortHandedTimeOnIce\",\n",
    "    \"powerPlayTimeOnIce\",\n",
    "]\n",
    "dfs[\"avg_skater_stats\"][\"gmean_stats\"] = gmean(dfs[\"avg_skater_stats\"], cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the geometric mean of positive-meaning stats in avg_goalie_stats\n",
    "cols = [\n",
    "    \"timeOnIce\",\n",
    "    \"assists\",\n",
    "    \"goals\",\n",
    "    \"pim\",\n",
    "    \"shots\",\n",
    "    \"saves\",\n",
    "    \"powerPlaySaves\",\n",
    "    \"shortHandedSaves\",\n",
    "    \"evenSaves\",\n",
    "    \"shortHandedShotsAgainst\",\n",
    "    \"evenShotsAgainst\",\n",
    "    \"powerPlayShotsAgainst\",\n",
    "    \"savePercentage\",\n",
    "    \"powerPlaySavePercentage\",\n",
    "    \"evenStrengthSavePercentage\",\n",
    "]\n",
    "dfs[\"avg_goalie_stats\"][\"gmean_stats\"] = gmean(dfs[\"avg_goalie_stats\"], cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate per-venue average game stats\n",
    "\n",
    "# Get stat totals per game\n",
    "df_shots = dfs[\"game_teams_stats\"][[\"game_id\", \"goals\", \"shots\", \"hits\"]]\n",
    "df_shots = df_shots.groupby(\"game_id\").sum(numeric_only=True)\n",
    "df_shots = df_shots.reset_index()\n",
    "df_shots.rename(columns={\"shots\": \"totalShots\"})\n",
    "# Get game venue\n",
    "df_venue = dfs[\"game\"][[\"game_id\", \"venue\"]]\n",
    "# Join game stats to venue\n",
    "df = dd.merge(df_shots, df_venue)\n",
    "df = df.groupby(\"venue\").mean(numeric_only=True)\n",
    "df = df.drop(\"game_id\", axis=1)\n",
    "df = df.reset_index()\n",
    "df[\"gmean_stats\"] = gmean(df, [\"goals\", \"shots\", \"hits\"])\n",
    "dfs[\"avg_venue_game_stats\"] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate event counts for period time bins to analyze period time trends\n",
    "\n",
    "# Divide the max of 1200 periodTime in 12 bins\n",
    "df = dfs[\"game_plays\"]\n",
    "df[\"periodTimeBin\"] = df[\"periodTime\"].map_partitions(pd.cut, 12)\n",
    "# Convert to str otherwise the groupby throws an exception\n",
    "df[\"periodTimeBin\"] = df[\"periodTimeBin\"].astype(\"str\")\n",
    "# Get counts of each event\n",
    "df = dfs[\"game_plays\"].groupby([\"periodTimeBin\", \"event\"]).count()\n",
    "df = df.reset_index()\n",
    "df = df[[\"periodTimeBin\", \"event\", \"play_id\"]]\n",
    "df = df.rename(columns={\"play_id\": \"count\"})\n",
    "# Format the periodTimeBin into human readable and sortable format\n",
    "value_map = {\n",
    "    \"(-1.2, 100.0]\": \"0000-0100\",\n",
    "    \"(100.0, 200.0]\": \"0100-0200\",\n",
    "    \"(200.0, 300.0]\": \"0200-0300\",\n",
    "    \"(300.0, 400.0]\": \"0300-0400\",\n",
    "    \"(400.0, 500.0]\": \"0400-0500\",\n",
    "    \"(500.0, 600.0]\": \"0500-0600\",\n",
    "    \"(600.0, 700.0]\": \"0600-0700\",\n",
    "    \"(700.0, 800.0]\": \"0700-0800\",\n",
    "    \"(800.0, 900.0]\": \"0800-0900\",\n",
    "    \"(900.0, 1000.0]\": \"0900-1000\",\n",
    "    \"(1000.0, 1100.0]\": \"1000-1100\",\n",
    "    \"(1100.0, 1200.0]\": \"1100-1200\",\n",
    "}\n",
    "df[\"periodTimeBin\"] = df[\"periodTimeBin\"].map(\n",
    "    value_map, meta=(\"periodTimeBin\", \"string[pyarrow]\")\n",
    ")\n",
    "dfs[\"period_event_count\"] = df\n",
    "# Cleanup columns\n",
    "dfs[\"game_plays\"] = dfs[\"game_plays\"].drop(\"periodTimeBin\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate counts for each penalty type per game\n",
    "df = dfs[\"game_plays\"][dfs[\"game_plays\"][\"event\"] == \"Penalty\"]\n",
    "df = df.groupby([\"game_id\", \"team_id_for\", \"secondaryType\"]).count()\n",
    "# Reset index to convert grouped object back into a DataFrame\n",
    "df = df.reset_index()\n",
    "df = df.rename(\n",
    "    columns={\n",
    "        \"team_id_for\": \"team_id\",  # The \"for\" team is the one receiving the penalty\n",
    "        \"secondaryType\": \"penaltyType\",\n",
    "        \"play_id\": \"count\",\n",
    "    }\n",
    ")\n",
    "df = df[[\"game_id\", \"team_id\", \"penaltyType\", \"count\"]]\n",
    "dfs[\"game_team_pen_count\"] = df\n",
    "\n",
    "# Calculate counts for fighting-related penalties per game\n",
    "vals = [\"Fighting\", \"Roughing\", \"Instigator - Misconduct\", \"Instigator\", \"Aggressor\"]\n",
    "df_fightpens = df[df[\"penaltyType\"].isin(vals)]\n",
    "df_fightpens = df_fightpens.groupby(\"game_id\").sum(numeric_only=True).reset_index()\n",
    "df_fightpens = df_fightpens.drop(\"team_id\", axis=1)\n",
    "# Join dates for easy plotting\n",
    "df_gamedates = dfs[\"game\"][[\"game_id\", \"date_time_GMT\"]]\n",
    "df = dd.merge(\n",
    "    df_fightpens, df_gamedates, how=\"left\", on=\"game_id\", suffixes=(\"_left\", \"_right\")\n",
    ")\n",
    "dfs[\"game_fight_penalties\"] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final cleanup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort values\n",
    "dfs[\"avg_goalie_stats\"] = dfs[\"avg_goalie_stats\"].sort_values(\"player_id\")\n",
    "dfs[\"avg_skater_stats\"] = dfs[\"avg_skater_stats\"].sort_values(\"player_id\")\n",
    "dfs[\"avg_teams_stats\"] = dfs[\"avg_teams_stats\"].sort_values(\"team_id\")\n",
    "dfs[\"avg_venue_game_stats\"] = dfs[\"avg_venue_game_stats\"].sort_values(\"venue\")\n",
    "dfs[\"game\"] = dfs[\"game\"].sort_values(\"game_id\")\n",
    "dfs[\"game_fight_penalties\"] = dfs[\"game_fight_penalties\"].sort_values(\"game_id\")\n",
    "dfs[\"game_goalie_stats\"] = dfs[\"game_goalie_stats\"].sort_values(\n",
    "    [\"game_id\", \"player_id\"]\n",
    ")\n",
    "dfs[\"game_goals\"] = dfs[\"game_goals\"].sort_values(\"play_id\")\n",
    "dfs[\"game_penalties\"] = dfs[\"game_penalties\"].sort_values(\"play_id\")\n",
    "dfs[\"game_plays\"] = dfs[\"game_plays\"].sort_values(\"play_id\")\n",
    "dfs[\"game_plays_players\"] = dfs[\"game_plays_players\"].sort_values(\n",
    "    [\"game_id\", \"play_id\", \"player_id\"]\n",
    ")\n",
    "dfs[\"game_scratches\"] = dfs[\"game_scratches\"].sort_values(\n",
    "    [\"game_id\", \"team_id\", \"player_id\"]\n",
    ")\n",
    "dfs[\"game_shifts\"] = dfs[\"game_shifts\"].sort_values(\n",
    "    [\"game_id\", \"player_id\", \"period\", \"shift_start\"]\n",
    ")\n",
    "dfs[\"game_skater_stats\"] = dfs[\"game_skater_stats\"].sort_values(\n",
    "    [\"game_id\", \"team_id\", \"player_id\"]\n",
    ")\n",
    "dfs[\"game_team_pen_count\"] = dfs[\"game_team_pen_count\"].sort_values(\n",
    "    [\"game_id\", \"team_id\", \"penaltyType\"]\n",
    ")\n",
    "dfs[\"game_teams_corfen\"] = dfs[\"game_teams_corfen\"].sort_values([\"game_id\", \"team_id\"])\n",
    "dfs[\"game_teams_stats\"] = dfs[\"game_teams_stats\"].sort_values([\"game_id\", \"team_id\"])\n",
    "dfs[\"period_event_count\"] = dfs[\"period_event_count\"].sort_values(\n",
    "    [\"periodTimeBin\", \"event\"]\n",
    ")\n",
    "dfs[\"player_info\"] = dfs[\"player_info\"].sort_values(\"player_id\")\n",
    "dfs[\"team_info\"] = dfs[\"team_info\"].sort_values(\"team_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parquet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to parquet for future manipulations\n",
    "for k in dfs:\n",
    "    fpath = os.path.join(EXPORT_PATH, k + \".parquet\")\n",
    "    start_time = time.time()\n",
    "    dfs[k].to_parquet(fpath, compression=\"zstd\", write_index=False)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    logger.info(f\"Exported {fpath} in {execution_time:.1f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lfRKIodLEbx3"
   },
   "outputs": [],
   "source": [
    "# Load the tables into an sql server\n",
    "# note that it takes about an hour due to data size\n",
    "def load_with_progress(df: dd.DataFrame, table_name: str, uri: str) -> None:\n",
    "    with tqdm(total=df.npartitions, desc=table_name) as pbar:\n",
    "        for i in range(df.npartitions):\n",
    "            if_exists = \"replace\" if i == 0 else \"append\"\n",
    "            df.get_partition(i).to_sql(\n",
    "                table_name,\n",
    "                uri,\n",
    "                index=False,\n",
    "                if_exists=if_exists,\n",
    "                chunksize=CHUNKSIZE,\n",
    "                method=\"multi\",\n",
    "            )\n",
    "            pbar.update(1)\n",
    "\n",
    "\n",
    "for k in dfs:\n",
    "    load_with_progress(dfs[k], k, sql_uri)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
