{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "j6vgaiLGPpxc"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import reduce\n",
    "from typing import List\n",
    "\n",
    "import dask\n",
    "import distributed\n",
    "import kagglehub\n",
    "import numpy as np\n",
    "from dask import dataframe as dd\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm\n",
    "\n",
    "# blocksize and partition size for dask dataframe\n",
    "# https://docs.dask.org/en/stable/generated/dask.dataframe.read_csv.html\n",
    "# https://docs.dask.org/en/stable/dataframe-create.html#read-from-csv\n",
    "DASK_BLOCKSIZE = \"64MB\"  # 64MB seems to be the best\n",
    "# name of the environment variable stored in ./.env for your sql uri\n",
    "# easily switch to cloud server here e.g. \"URI_AZURE\"\n",
    "ENV_SQL_URI = \"URI_PG\"\n",
    "# chunksize for to_sql. reduce value if you run out of memory\n",
    "CHUNKSIZE = 10000\n",
    "# path to export data\n",
    "EXPORT_PATH = \"./.export\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from ./.env\n",
    "is_dotenv = load_dotenv()\n",
    "if not is_dotenv:  # remind user to create a .env\n",
    "    raise Exception(f\"Please create a .env and define the {ENV_SQL_URI} variable\")\n",
    "# check if sql uri exists\n",
    "sql_uri = str(os.getenv(ENV_SQL_URI))\n",
    "if sql_uri == \"None\":\n",
    "    raise Exception(f\"Please define an sql uri on key {ENV_SQL_URI}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set temporary directory so it doesn't burn out my SSD\n",
    "dask.config.set({\"temporary_directory\": \"./.tmp\"})  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the dask distributed client\n",
    "# click on the link in output to see the dashboard\n",
    "distributed.Client()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset to project folder\n",
    "# https://github.com/Kaggle/kagglehub/issues/175\n",
    "os.environ[\"KAGGLEHUB_CACHE\"] = \"./.kaggle\"\n",
    "dataset_path = kagglehub.dataset_download(\"martinellis/nhl-game-data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# Define column dtypes for each file\n",
    "\n",
    "# dtype shortcuts\n",
    "DTYPE_BOOL = \"bool[pyarrow]\"\n",
    "DTYPE_INT = \"int64[pyarrow]\"  # note that pyarrow ints are nullable\n",
    "DTYPE_INT8 = \"int8[pyarrow]\"\n",
    "DTYPE_INT16 = \"int16[pyarrow]\"\n",
    "DTYPE_INT32 = \"int32[pyarrow]\"\n",
    "DTYPE_FLOAT = \"float64[pyarrow]\"\n",
    "DTYPE_STRING = \"string[pyarrow]\"\n",
    "DTYPE_DATETIME = \"timestamp[s][pyarrow]\"\n",
    "\n",
    "# full definitions\n",
    "dtype_defs = {}\n",
    "dtype_defs[\"game.csv\"] = {\n",
    "    \"game_id\": DTYPE_INT,\n",
    "    \"season\": DTYPE_INT,\n",
    "    \"type\": DTYPE_STRING,\n",
    "    \"date_time_GMT\": DTYPE_DATETIME,\n",
    "    \"away_team_id\": DTYPE_INT8,\n",
    "    \"home_team_id\": DTYPE_INT8,\n",
    "    \"away_goals\": DTYPE_INT8,\n",
    "    \"home_goals\": DTYPE_INT8,\n",
    "    \"outcome\": DTYPE_STRING,\n",
    "    \"home_rink_side_start\": DTYPE_STRING,\n",
    "    \"venue\": DTYPE_STRING,\n",
    "    \"venue_link\": DTYPE_STRING,\n",
    "    \"venue_time_zone_id\": DTYPE_STRING,\n",
    "    \"venue_time_zone_offset\": DTYPE_INT8,\n",
    "    \"venue_time_zone_tz\": DTYPE_STRING,\n",
    "}\n",
    "dtype_defs[\"game_goalie_stats.csv\"] = {\n",
    "    \"game_id\": DTYPE_INT,\n",
    "    \"player_id\": DTYPE_INT32,\n",
    "    \"team_id\": DTYPE_INT8,\n",
    "    \"timeOnIce\": DTYPE_INT,\n",
    "    \"assists\": DTYPE_INT,\n",
    "    \"goals\": DTYPE_INT,\n",
    "    \"pim\": DTYPE_INT,\n",
    "    \"shots\": DTYPE_INT,\n",
    "    \"saves\": DTYPE_INT,\n",
    "    \"powerPlaySaves\": DTYPE_INT,\n",
    "    \"shortHandedSaves\": DTYPE_INT,\n",
    "    \"evenSaves\": DTYPE_INT,\n",
    "    \"shortHandedShotsAgainst\": DTYPE_INT,\n",
    "    \"evenShotsAgainst\": DTYPE_INT,\n",
    "    \"powerPlayShotsAgainst\": DTYPE_INT,\n",
    "    \"decision\": DTYPE_STRING,\n",
    "    \"savePercentage\": DTYPE_FLOAT,\n",
    "    \"powerPlaySavePercentage\": DTYPE_FLOAT,\n",
    "    \"evenStrengthSavePercentage\": DTYPE_FLOAT,\n",
    "}\n",
    "dtype_defs[\"game_goals.csv\"] = {\n",
    "    \"play_id\": DTYPE_STRING,\n",
    "    \"strength\": DTYPE_STRING,\n",
    "    \"gameWinningGoal\": DTYPE_BOOL,\n",
    "    \"emptyNet\": DTYPE_BOOL,\n",
    "}\n",
    "dtype_defs[\"game_officials.csv\"] = {\n",
    "    \"game_id\": DTYPE_INT,\n",
    "    \"official_name\": DTYPE_STRING,\n",
    "    \"official_type\": DTYPE_STRING,\n",
    "}\n",
    "dtype_defs[\"game_penalties.csv\"] = {\n",
    "    \"play_id\": DTYPE_STRING,\n",
    "    \"penaltySeverity\": DTYPE_STRING,\n",
    "    \"penaltyMinutes\": DTYPE_INT,\n",
    "}\n",
    "dtype_defs[\"game_plays.csv\"] = {\n",
    "    \"play_id\": DTYPE_STRING,\n",
    "    \"game_id\": DTYPE_INT,\n",
    "    \"team_id_for\": DTYPE_INT8,\n",
    "    \"team_id_against\": DTYPE_INT8,\n",
    "    \"event\": DTYPE_STRING,\n",
    "    \"secondaryType\": DTYPE_STRING,\n",
    "    \"x\": DTYPE_INT8,\n",
    "    \"y\": DTYPE_INT8,\n",
    "    \"period\": DTYPE_INT8,\n",
    "    \"periodType\": DTYPE_STRING,\n",
    "    \"periodTime\": DTYPE_INT16,\n",
    "    \"periodTimeRemaining\": DTYPE_INT16,\n",
    "    \"dateTime\": DTYPE_DATETIME,\n",
    "    \"goals_away\": DTYPE_INT8,\n",
    "    \"goals_home\": DTYPE_INT8,\n",
    "    \"description\": DTYPE_STRING,\n",
    "    \"st_x\": DTYPE_INT8,\n",
    "    \"st_y\": DTYPE_INT8,\n",
    "}\n",
    "dtype_defs[\"game_plays_players.csv\"] = {\n",
    "    \"play_id\": DTYPE_STRING,\n",
    "    \"game_id\": DTYPE_INT,\n",
    "    \"player_id\": DTYPE_INT32,\n",
    "    \"playerType\": DTYPE_STRING,\n",
    "}\n",
    "dtype_defs[\"game_scratches.csv\"] = {\n",
    "    \"game_id\": DTYPE_INT,\n",
    "    \"team_id\": DTYPE_INT8,\n",
    "    \"player_id\": DTYPE_INT32,\n",
    "}\n",
    "dtype_defs[\"game_shifts.csv\"] = {\n",
    "    \"game_id\": DTYPE_INT,\n",
    "    \"player_id\": DTYPE_INT32,\n",
    "    \"period\": DTYPE_INT8,\n",
    "    \"shift_start\": DTYPE_INT16,\n",
    "    \"shift_end\": DTYPE_INT16,\n",
    "}\n",
    "dtype_defs[\"game_skater_stats.csv\"] = {\n",
    "    \"game_id\": DTYPE_INT,\n",
    "    \"player_id\": DTYPE_INT32,\n",
    "    \"team_id\": DTYPE_INT8,\n",
    "    \"timeOnIce\": DTYPE_INT16,\n",
    "    \"assists\": DTYPE_INT8,\n",
    "    \"goals\": DTYPE_INT8,\n",
    "    \"shots\": DTYPE_INT8,\n",
    "    \"hits\": DTYPE_INT8,\n",
    "    \"powerPlayGoals\": DTYPE_INT8,\n",
    "    \"powerPlayAssists\": DTYPE_INT8,\n",
    "    \"penaltyMinutes\": DTYPE_INT8,\n",
    "    \"faceOffWins\": DTYPE_INT8,\n",
    "    \"faceoffTaken\": DTYPE_INT8,\n",
    "    \"takeaways\": DTYPE_INT8,\n",
    "    \"giveaways\": DTYPE_INT8,\n",
    "    \"shortHandedGoals\": DTYPE_INT8,\n",
    "    \"shortHandedAssists\": DTYPE_INT8,\n",
    "    \"blocked\": DTYPE_INT8,\n",
    "    \"plusMinus\": DTYPE_INT8,\n",
    "    \"evenTimeOnIce\": DTYPE_INT16,\n",
    "    \"shortHandedTimeOnIce\": DTYPE_INT16,\n",
    "    \"powerPlayTimeOnIce\": DTYPE_INT16,\n",
    "}\n",
    "dtype_defs[\"game_teams_stats.csv\"] = {\n",
    "    \"game_id\": DTYPE_INT,\n",
    "    \"team_id\": DTYPE_INT8,\n",
    "    \"HoA\": DTYPE_STRING,\n",
    "    \"won\": \"bool\",\n",
    "    \"settled_in\": DTYPE_STRING,\n",
    "    \"head_coach\": DTYPE_STRING,\n",
    "    \"goals\": DTYPE_INT,\n",
    "    \"shots\": DTYPE_INT,\n",
    "    \"hits\": DTYPE_INT,\n",
    "    \"pim\": DTYPE_INT,\n",
    "    \"powerPlayOpportunities\": DTYPE_INT,\n",
    "    \"powerPlayGoals\": DTYPE_INT,\n",
    "    \"faceOffWinPercentage\": DTYPE_FLOAT,\n",
    "    \"giveaways\": DTYPE_INT,\n",
    "    \"takeaways\": DTYPE_INT,\n",
    "    \"blocked\": DTYPE_INT,\n",
    "    \"startRinkSide\": DTYPE_STRING,\n",
    "}\n",
    "dtype_defs[\"player_info.csv\"] = {\n",
    "    \"player_id\": DTYPE_INT32,\n",
    "    \"firstName\": DTYPE_STRING,\n",
    "    \"lastName\": DTYPE_STRING,\n",
    "    \"nationality\": DTYPE_STRING,\n",
    "    \"birthCity\": DTYPE_STRING,\n",
    "    \"primaryPosition\": DTYPE_STRING,\n",
    "    \"birthDate\": DTYPE_DATETIME,\n",
    "    \"birthStateProvince\": DTYPE_STRING,\n",
    "    \"height\": DTYPE_STRING,\n",
    "    \"height_cm\": DTYPE_FLOAT,\n",
    "    \"weight\": DTYPE_INT,\n",
    "    \"shootsCatches\": DTYPE_STRING,\n",
    "}\n",
    "dtype_defs[\"team_info.csv\"] = {\n",
    "    \"team_id\": DTYPE_INT8,\n",
    "    \"franchiseId\": DTYPE_INT,\n",
    "    \"shortName\": DTYPE_STRING,\n",
    "    \"teamName\": DTYPE_STRING,\n",
    "    \"abbreviation\": DTYPE_STRING,\n",
    "    \"link\": DTYPE_STRING,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HZint-P3TbkH",
    "outputId": "b1571a3b-dc0b-4d33-b672-bce2f553e30e"
   },
   "outputs": [],
   "source": [
    "# Read all csv files from data directory\n",
    "dfs: dict[str, dd.DataFrame] = {}\n",
    "for n in os.listdir(dataset_path):\n",
    "    if n.endswith(\".csv\"):\n",
    "        if n in dtype_defs:\n",
    "            print(f\"âœ… reading {n} with dtypes\")\n",
    "            fpath = os.path.join(dataset_path, n)\n",
    "            dfs[n.split(\".\")[0]] = dd.read_csv(\n",
    "                fpath,\n",
    "                blocksize=DASK_BLOCKSIZE,\n",
    "                dtype=dtype_defs[n],\n",
    "                engine=\"pyarrow\",\n",
    "                dtype_backend=\"pyarrow\",\n",
    "            )\n",
    "        else:\n",
    "            print(f\"âŒ reading {n} without dtypes\")\n",
    "            dfs[n.split(\".\")[0]] = dd.read_csv(os.path.join(dataset_path, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print partitions\n",
    "for k in dfs:\n",
    "    print(f\"âœ‚ï¸ npartitions {k}: {dfs[k].npartitions}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qc3UW3wMaXit",
    "outputId": "eefa358e-b561-4ddc-aa73-9ec65748483b",
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "# compute to verify types are compatible\n",
    "# don't need to run this once types are finalized\n",
    "\n",
    "# for n in dfs:\n",
    "#     try:\n",
    "#         dfs[n].compute()\n",
    "#         print(f\"âœ… {n} passed compute\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"âŒ {n} did not pass compute due to:\")\n",
    "#         print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop duplicate rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicates(df: dd.DataFrame, subset: str) -> dd.DataFrame:\n",
    "    \"\"\"Drop duplicates and return the dataframe\"\"\"\n",
    "    rows_before = df.shape[0].compute()\n",
    "    df_deduped = df.drop_duplicates(subset=subset)\n",
    "    rows_after = df_deduped.shape[0].compute()\n",
    "    print(f\"ðŸ—‘ï¸ dropped {rows_before - rows_after} rows using {subset}\")\n",
    "    return df_deduped\n",
    "\n",
    "\n",
    "# I have verified that data with the same key are duplicate rows\n",
    "dfs[\"game\"] = drop_duplicates(dfs[\"game\"], \"game_id\")\n",
    "dfs[\"game_plays\"] = drop_duplicates(dfs[\"game_plays\"], \"play_id\")\n",
    "dfs[\"player_info\"] = drop_duplicates(dfs[\"player_info\"], \"player_id\")\n",
    "dfs[\"team_info\"] = drop_duplicates(dfs[\"team_info\"], \"team_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop unreferenced data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop unreferenced data that is unrecoverable\n",
    "def drop_unreferenced(\n",
    "    df_foreign: dd.DataFrame,\n",
    "    df_foreign_col: str,\n",
    "    df_primary: dd.DataFrame,\n",
    "    df_primary_col: str,\n",
    ") -> dd.DataFrame:\n",
    "    \"\"\"Drop unreferenced keys and returns the dataframe\"\"\"\n",
    "    rows_before = df_foreign.shape[0].compute()\n",
    "    # reduce memory usage with partitions\n",
    "    selections = []\n",
    "    for i in range(df_primary.npartitions):\n",
    "        selections.append(\n",
    "            df_foreign[df_foreign_col].isin(df_primary[df_primary_col].get_partition(i))\n",
    "        )\n",
    "    selector = reduce(lambda x, y: x | y, selections)\n",
    "    df_foreign = df_foreign[selector]\n",
    "    rows_after = df_foreign.shape[0].compute()\n",
    "    print(f\"ðŸ—‘ï¸ dropped {rows_before - rows_after} rows on {df_foreign_col}\")\n",
    "    return df_foreign\n",
    "\n",
    "\n",
    "dfs[\"game_goalie_stats\"] = drop_unreferenced(\n",
    "    dfs[\"game_goalie_stats\"], \"team_id\", dfs[\"team_info\"], \"team_id\"\n",
    ")\n",
    "dfs[\"game_plays_players\"] = drop_unreferenced(\n",
    "    dfs[\"game_plays_players\"], \"play_id\", dfs[\"game_plays\"], \"play_id\"\n",
    ")\n",
    "dfs[\"game_skater_stats\"] = drop_unreferenced(\n",
    "    dfs[\"game_skater_stats\"], \"team_id\", dfs[\"team_info\"], \"team_id\"\n",
    ")\n",
    "dfs[\"game_teams_stats\"] = drop_unreferenced(\n",
    "    dfs[\"game_teams_stats\"], \"team_id\", dfs[\"team_info\"], \"team_id\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HACK: to_parquet throws an error without this even though the dtypes seem correct\n",
    "dfs[\"game_teams_stats\"][\"won\"] = dfs[\"game_teams_stats\"][\"won\"].astype(\"bool[pyarrow]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add summary statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmean(df: dd.DataFrame, cols: List[str]):\n",
    "    \"\"\"Calculate the geometric mean across a list of columns `cols` in `df`\"\"\"\n",
    "    return df[cols].prod(axis=1) ** (1 / len(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting 'won' from bool to int allows us to perform mean() and calculate the win rate\n",
    "dfs[\"game_teams_stats\"][\"winRate\"] = dfs[\"game_teams_stats\"][\"won\"].astype(\n",
    "    \"int8[pyarrow]\"\n",
    ")\n",
    "\n",
    "# Generate average team stats from per-game team stats\n",
    "dfs[\"avg_teams_stats\"] = (\n",
    "    dfs[\"game_teams_stats\"].groupby(\"team_id\").mean(numeric_only=True)\n",
    ")\n",
    "dfs[\"avg_teams_stats\"] = dfs[\"avg_teams_stats\"].drop(\"game_id\", axis=1)\n",
    "dfs[\"avg_teams_stats\"] = dfs[\"avg_teams_stats\"].reset_index()\n",
    "\n",
    "# Drop winRate from per-game stats as it is only meaningful as a mean in the average stats\n",
    "dfs[\"game_teams_stats\"] = dfs[\"game_teams_stats\"].drop(\"winRate\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating Corsi and Fenwick per game\n",
    "# Self-join to allow cross-comparison of shots\n",
    "cols = [\"game_id\", \"team_id\", \"shots\", \"blocked\"]\n",
    "df_a = dfs[\"game_teams_stats\"][cols].rename(columns={\"team_id\": \"team_id_a\"})\n",
    "df_b = dfs[\"game_teams_stats\"][cols].rename(columns={\"team_id\": \"team_id_b\"})\n",
    "df = dd.merge(df_a, df_b, on=\"game_id\", suffixes=(\"_a\", \"_b\"))\n",
    "# Cleanup duplicates after self-join\n",
    "df = df[df[\"team_id_a\"] != df[\"team_id_b\"]]\n",
    "# Calculate Corsi and Fenwick\n",
    "df[\"corsi_a\"] = df[\"shots_a\"] - df[\"shots_b\"]\n",
    "df[\"fenwick_a\"] = df[\"shots_a\"] - df[\"blocked_a\"] - (df[\"shots_b\"] - df[\"blocked_b\"])\n",
    "# Use smallest int that fits the min and max to save memory\n",
    "df[\"corsi_a\"] = df[\"corsi_a\"].astype(\"int8[pyarrow]\")\n",
    "df[\"fenwick_a\"] = df[\"fenwick_a\"].astype(\"int8[pyarrow]\")\n",
    "# Cleanup redundant columns\n",
    "cols = [\"shots_a\", \"blocked_a\", \"team_id_b\", \"shots_b\", \"blocked_b\"]\n",
    "df = df.drop(cols, axis=1)\n",
    "# Remove suffixes\n",
    "cols = {\"team_id_a\": \"team_id\", \"corsi_a\": \"corsi\", \"fenwick_a\": \"fenwick\"}\n",
    "df = df.rename(columns=cols)\n",
    "# Save to dataframe collection\n",
    "dfs[\"game_teams_corfen\"] = df\n",
    "\n",
    "# Compute the average Corsi and Fenwick per team and save to avg_teams_stats\n",
    "df = df.groupby(\"team_id\").mean(numeric_only=True)\n",
    "df = df.reset_index()\n",
    "df[\"team_id\"] = df[\"team_id\"].astype(\"int64[pyarrow]\")\n",
    "df = df.drop(\"game_id\", axis=1)\n",
    "dfs[\"avg_teams_stats\"] = dfs[\"avg_teams_stats\"].join(df, on=\"team_id\", rsuffix=\"_right\")\n",
    "dfs[\"avg_teams_stats\"] = dfs[\"avg_teams_stats\"].drop(\"team_id_right\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate average skater stats from per-game skater stats\n",
    "dfs[\"avg_skater_stats\"] = (\n",
    "    dfs[\"game_skater_stats\"].groupby(\"player_id\").mean(numeric_only=True)\n",
    ")\n",
    "dfs[\"avg_skater_stats\"] = dfs[\"avg_skater_stats\"].drop([\"game_id\", \"team_id\"], axis=1)\n",
    "dfs[\"avg_skater_stats\"] = dfs[\"avg_skater_stats\"].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the decision string to int to allow the mean to calculate the win rate\n",
    "def decision_to_num(decision):\n",
    "    if isinstance(decision, str):\n",
    "        if decision == \"W\":\n",
    "            return 1\n",
    "        elif decision == \"L\":\n",
    "            return 0\n",
    "        else:\n",
    "            raise Exception(f\"Unexpected string '{decision}'\")\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "dfs[\"game_goalie_stats\"][\"decisionWinRate\"] = dfs[\"game_goalie_stats\"][\n",
    "    \"decision\"\n",
    "].apply(decision_to_num, meta=(0, \"int8[pyarrow]\"))\n",
    "\n",
    "# Generate average goalie stats from per-game goalie stats\n",
    "dfs[\"avg_goalie_stats\"] = (\n",
    "    dfs[\"game_goalie_stats\"].groupby(\"player_id\").mean(numeric_only=True)\n",
    ")\n",
    "dfs[\"avg_goalie_stats\"] = dfs[\"avg_goalie_stats\"].drop([\"game_id\", \"team_id\"], axis=1)\n",
    "dfs[\"avg_goalie_stats\"] = dfs[\"avg_goalie_stats\"].reset_index()\n",
    "\n",
    "# Drop the redundant column from per-game stats\n",
    "dfs[\"game_goalie_stats\"] = dfs[\"game_goalie_stats\"].drop(\"decisionWinRate\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the geometric mean of positive-meaning stats in avg_teams_stats\n",
    "# Note that we cannot include the Corsi and Fenwick as they have negative values\n",
    "cols = [\n",
    "    \"goals\",\n",
    "    \"shots\",\n",
    "    \"hits\",\n",
    "    \"powerPlayOpportunities\",\n",
    "    \"powerPlayGoals\",\n",
    "    \"faceOffWinPercentage\",\n",
    "    \"takeaways\",\n",
    "    \"blocked\",\n",
    "]\n",
    "dfs[\"avg_teams_stats\"][\"gmean_stats\"] = gmean(dfs[\"avg_teams_stats\"], cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the geometric mean of positive-meaning stats in avg_skater_stats\n",
    "cols = [\n",
    "    \"timeOnIce\",\n",
    "    \"assists\",\n",
    "    \"goals\",\n",
    "    \"shots\",\n",
    "    \"hits\",\n",
    "    \"powerPlayGoals\",\n",
    "    \"powerPlayAssists\",\n",
    "    \"penaltyMinutes\",\n",
    "    \"faceOffWins\",\n",
    "    \"faceoffTaken\",\n",
    "    \"takeaways\",\n",
    "    \"shortHandedGoals\",\n",
    "    \"shortHandedAssists\",\n",
    "    \"blocked\",\n",
    "    \"plusMinus\",\n",
    "    \"evenTimeOnIce\",\n",
    "    \"shortHandedTimeOnIce\",\n",
    "    \"powerPlayTimeOnIce\",\n",
    "]\n",
    "dfs[\"avg_skater_stats\"][\"gmean_stats\"] = gmean(dfs[\"avg_skater_stats\"], cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the geometric mean of positive-meaning stats in avg_goalie_stats\n",
    "cols = [\n",
    "    \"timeOnIce\",\n",
    "    \"assists\",\n",
    "    \"goals\",\n",
    "    \"pim\",\n",
    "    \"shots\",\n",
    "    \"saves\",\n",
    "    \"powerPlaySaves\",\n",
    "    \"shortHandedSaves\",\n",
    "    \"evenSaves\",\n",
    "    \"shortHandedShotsAgainst\",\n",
    "    \"evenShotsAgainst\",\n",
    "    \"powerPlayShotsAgainst\",\n",
    "    \"savePercentage\",\n",
    "    \"powerPlaySavePercentage\",\n",
    "    \"evenStrengthSavePercentage\",\n",
    "]\n",
    "dfs[\"avg_goalie_stats\"][\"gmean_stats\"] = gmean(dfs[\"avg_goalie_stats\"], cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to parquet for future manipulations\n",
    "for k in dfs:\n",
    "    fpath = os.path.join(EXPORT_PATH, k + \".parquet\")\n",
    "    dfs[k].to_parquet(fpath, compression=\"zstd\", write_index=True)\n",
    "    print(f\"ðŸ“‚ exported {fpath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lfRKIodLEbx3"
   },
   "outputs": [],
   "source": [
    "# Load the tables into an sql server\n",
    "# note that it takes about an hour due to data size\n",
    "def load_with_progress(df: dd.DataFrame, table_name: str, uri: str) -> None:\n",
    "    with tqdm(total=df.npartitions, desc=table_name) as pbar:\n",
    "        for i in range(df.npartitions):\n",
    "            if_exists = \"replace\" if i == 0 else \"append\"\n",
    "            df.get_partition(i).to_sql(\n",
    "                table_name,\n",
    "                uri,\n",
    "                index=True,\n",
    "                if_exists=if_exists,\n",
    "                chunksize=CHUNKSIZE,\n",
    "                method=\"multi\",\n",
    "            )\n",
    "            pbar.update(1)\n",
    "\n",
    "\n",
    "for k in dfs:\n",
    "    load_with_progress(dfs[k], k, sql_uri)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
